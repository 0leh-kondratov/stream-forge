## Часть I: Предпосылки и цели проекта

### 1.1. Проблема: Особенности криптовалютных данных

Криптовалютные данные характеризуются высокой волатильностью, круглосуточной доступностью и значительными объемами генерации (обновления книги ордеров, тысячи сделок в секунду, минутные свечи). Это создает сложности в их сборе, обработке и извлечении значимой информации.

Основные сложности включали:
*   **Много источников:** Данные поступают как по REST API (для исторических данных), так и по WebSocket (для данных в реальном времени), что требует интеграции различных источников.
*   **Масштаб и скорость:** Система должна выдерживать пиковые нагрузки и обрабатывать большие потоки данных без потерь.
*   **Надежность:** Необходимо обеспечить сохранность данных и быстрое восстановление системы в случае сбоев.
*   **Сложность оркестрации:** Для запуска сложных последовательностей задач, таких как «загрузка -> сохранение -> построение графа -> обучение модели», требовалась надежная система оркестрации.

### 1.2. Концепция: Событийно-ориентированная платформа

В основе **StreamForge** лежит концепция событийно-ориентированной платформы для обработки данных.

Основной принцип архитектуры — отсутствие прямых вызовов между сервисами. Взаимодействие осуществляется через брокер сообщений **Apache Kafka**. Каждый сервис публикует информацию в общую шину, а другие сервисы, заинтересованные в этих данных, подписываются на соответствующие топики. Такой подход обеспечивает гибкость, независимость и взаимозаменяемость компонентов. `queue-manager` выступает в роли оркестратора, объявляя о задачах, которые затем подхватываются соответствующими исполнителями.

Это способствует масштабируемости, адаптивности и отказоустойчивости системы.

### 1.3. Цели проекта

1.  **Создание единого источника данных:** Централизованный сбор, очистка и хранение рыночных данных для обеспечения быстрого и удобного доступа.
2.  **Среда для Data Science:** Предоставление окружения для разработки и тестирования моделей, включая графовые нейронные сети (GNN).
3.  **Основа для алгоритмической торговли:** Построение производительного и надежного конвейера данных в качестве фундамента для торговых систем.
4.  **Автоматизация процессов:** Минимизация ручных операций в процессах сбора и анализа данных.

### 1.4. Практические сценарии использования

*   **Сценарий 1: Обучение модели на исторических данных.**
    *   **Задача:** Обучить GNN-модель на данных по сделкам и 5-минутным свечам для `BTCUSDT` за последний месяц.
    *   **Решение:** Отправляется запрос в `queue-manager`, описывающий весь процесс. Система запускает Kubernetes Jobs: `loader-producer` (загружает данные в Kafka), `arango-connector` (сохраняет в ArangoDB), `graph-builder` (строит граф) и `gnn-trainer` (обучает модель).

*   **Сценарий 2: Мониторинг рынка в реальном времени.**
    *   **Задача:** Получать поток сделок и книгу ордеров для `ETHUSDT` в реальном времени.
    *   **Решение:** Запускается `loader-ws`, который подключается к WebSocket и отправляет данные в Kafka. Визуализатор (в разработке) подписывается на эти данные и отображает их на дашборде.

*   **Сценарий 3: Быстрый анализ данных.**
    *   **Задача:** Проверить гипотезу о корреляции объемов торгов и волатильности.
    *   **Решение:** С помощью `Jupyter Server` (который также является частью системы) выполняется подключение к ArangoDB для анализа данных, которые StreamForge уже собрал и обработал.