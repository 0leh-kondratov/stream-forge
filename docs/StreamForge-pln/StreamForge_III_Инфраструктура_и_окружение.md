## Часть III: Инфраструктура и окружение

Платформа **StreamForge** развернута в рамках высокопроизводительной локальной лаборатории и базируется на фундаменте открытых технологий, сфокусированных на обеспечении надежности, масштабируемости и управляемости. Центральным элементом инфраструктуры является кластер **Kubernetes**, функционирующий в виртуализированной среде и использующий современные методологии DevOps и архитектурные паттерны облачных вычислений.

### Глава 5: Основы платформы: Kubernetes и виртуализация

#### 5.1. Фундамент: Proxmox VE

В качестве базового уровня виртуализации используется **Proxmox VE** — зрелая платформа корпоративного класса, обеспечивающая изоляцию вычислительных сред, высокую доступность и централизованное управление ресурсами. Виртуальные машины, развернутые на Proxmox VE, служат хостами для узлов кластера Kubernetes.

#### 5.2. Развертывание кластера: Kubespray

Развёртывание Kubernetes-кластера осуществляется с использованием **Kubespray** — автоматизированного инструмента, рекомендованного CNCF для production-сред. Kubespray обеспечивает идемпотентный и повторяемый процесс настройки, включая установку компонентов control-plane, конфигурирование сетевой топологии и интеграцию с TLS-решениями, что гарантирует консистентность и воспроизводимость развертывания.

#### 5.3. Сетевая инфраструктура

Сетевая инфраструктура StreamForge спроектирована с учетом обеспечения высокой надежности и адаптивности, ориентирована на отказоустойчивость и прозрачность внешнего доступа:

- **kube-vip** обеспечивает виртуальный IP-адрес для высокодоступного доступа (HA) к Kubernetes API, что позволяет автоматически перенаправлять трафик в случае сбоя управляющего узла.
- **MetalLB** версии `0.14.9` используется в режиме Layer2 для поддержки сервисов типа `LoadBalancer` в bare-metal окружении, что исключает необходимость в аппаратном балансировщике нагрузки.

#### 5.4. Ingress и Gateway API: Управление трафиком

Для управления входящим трафиком в StreamForge применяются два Ingress-контроллера, обеспечивающие гибкую маршрутизацию и отказоустойчивость:

- **Traefik** (v36.1.0) — основной Ingress-контроллер, использующий новый **Gateway API** для декларативной маршрутизации и управления трафиком на уровнях L7 (HTTP/HTTPS) и L4 (TCP/UDP).
- **ingress-nginx** (v4.12.1) — резервный Ingress-контроллер, обеспечивающий совместимость и дополнительную отказоустойчивость.

Настройки включают:
- TLS через `cert-manager` и внутренний CA `homelab-ca-issuer`
- Внешний IP: `192.168.1.153`
- Хранилище ACME: 1Gi NFS
- Мониторинг через `/dashboard`
- Поддержка TCP-сервисов (`ssh`, `kafka`)

#### 5.5. DNS и TLS

- **Technitium DNS Server** предоставляет локальный разрешитель с поддержкой произвольных DNS-зон, включая `*.dmz.home`, обеспечивая доступ к сервисам по человекочитаемым именам.
- **cert-manager** автоматизирует управление TLS-сертификатами, снижая риск ошибок и усиливая безопасность коммуникаций между компонентами.

##### Скрипт `script.sh` для генерации TLS-сертификатов

Сценарий `/platform/base/cert/script.sh` автоматизирует цикл создания TLS-сертификатов, включая:
1. Настройку параметров генерации и хранения;
2. Создание CSR с SAN-полями (FQDN + IP);
3. Формирование ресурса `CertificateRequest` и подачу его в `cert-manager`;
4. Ожидание выполнения и сохранение PEM-файлов;
5. Валидацию сертификата с помощью `openssl`.

---

### Глава 6: Управление данными: Стратегии хранения и доступа

Устойчивое хранение данных является критически важным аспектом для обеспечения долговременной аналитики и эффективного обучения моделей. В StreamForge хранилище данных сегментировано по функциональным зонам для оптимизации производительности и доступности.

#### 6.1. Обзор Storage-решений

- **Linstor Piraeus** — отказоустойчивое блочное хранилище (RWO) для критически важных сервисов, таких как PostgreSQL и ArangoDB, обеспечивающее высокую доступность и целостность данных.
- **GlusterFS** и **NFS Subdir External Provisioner** (v4.0.18) — используются для предоставления общих томов с режимом RWX (ReadWriteMany), что идеально подходит для JupyterHub и совместно используемых данных. Основной путь доступа: `192.168.1.6:/data0/k2`.

#### 6.2. Объектное хранилище Minio

**Minio** — S3-совместимое объектное хранилище, используемое для:
- хранения артефактов моделей (GNN, PPO),
- резервного копирования сервисов и метаданных.

Обеспечивает высокую доступность и интеграцию с Kubernetes через StatefulSet.

---

### Глава 7: Платформа данных: Управление информацией

#### 7.1. Strimzi Kafka Operator

**Strimzi** обеспечивает полный жизненный цикл Apache Kafka в среде Kubernetes, включая развертывание, обновление, конфигурирование топиков, шифрование и мониторинг. Интеграция осуществляется посредством ресурсов `KafkaUser`, `KafkaTopic` и `KafkaConnect`.

#### 7.2. ArangoDB: Мультимодельная база данных

ArangoDB представляет собой мультимодельную базу данных, которая позволяет эффективно комбинировать документо- и граф-ориентированные модели данных в рамках единого движка:
- **Документы**: используются для хранения исторических свечей и событий, обеспечивая гибкость и масштабируемость.
- **Графы**: применяются для описания сложных взаимосвязей между активами и торговыми операциями, что является критически важным для функционирования графовых нейронных сетей (GNN).

#### 7.3. PostgreSQL (Zalando Operator)

**Zalando Operator** обеспечивает развертывание и управление кластерами PostgreSQL с высокой доступностью, автоматическим резервным копированием и механизмом failover. Данное решение используется для хранения структурированных таблиц, содержащих информацию о рентабельности инвестиций (ROI), логи действий агентов и метаданные об экспериментах.

#### 7.4. Автомасштабирование с KEDA

**KEDA** (Kubernetes Event-driven Autoscaling) — это компонент, который динамически масштабирует количество consumer-подов в зависимости от объема сообщений в Apache Kafka. Это обеспечивает оптимальную адаптацию к пиковым и низким нагрузкам без необходимости ручного вмешательства, что приводит к снижению операционных затрат и минимизации времени простоя (idle-time).

#### 7.5. Kafka UI

**Kafka UI** — это веб-интерфейс, разработанный `provectuslabs`, который предоставляет интуитивно понятный визуальный контроль над топиками, группами потребителей (consumer-groups), пользователями и сообщениями в Apache Kafka.

Параметры:
- Доступ по адресу `https://kafka-ui.dmz.home`
- Интеграция через SASL_SSL (SCRAM-SHA-512)
- Подключение к кластеру `k3`
- Запуск на `k2w-7`, 1 реплика

---

### Глава 8: Мониторинг и наблюдаемость: Комплексный контроль системы

Для обеспечения стабильной работы и оперативного реагирования на инциденты в StreamForge реализована комплексная система мониторинга и наблюдаемости.

#### 8.1. Метрики: Prometheus, NodeExporter, cAdvisor

- **Prometheus** — система сбора и хранения временных рядов, используемая для агрегации системных и прикладных метрик.
- **cAdvisor** — инструмент для мониторинга ресурсов и производительности контейнеров.
- **NodeExporter** — экспортер метрик операционной системы и хоста.

Компоненты:
- kube-prometheus-stack `v71.1.0`
- TLS + Ingress для Prometheus (`prometheus.dmz.home`) и Grafana (`grafana.dmz.home`)
- Persistent volumes: Prometheus — 20Gi, Grafana — 1Gi

#### 8.2. Логи: Fluent-bit, Elasticsearch, Kibana

**EFK стек** (Elasticsearch, Fluent-bit, Kibana) используется для централизованного сбора, маршрутизации и анализа логов в системе StreamForge:

- **Fluent-bit** применяет Lua-фильтр для динамического создания индексов на основе тега (e.g. `internal-myapp-2025.08.07`), обеспечивая гибкость в индексации логов.
- **Elasticsearch** обеспечивает полнотекстовый поиск, агрегации и хранение логов.
- **Kibana** визуализирует логи, предоставляя удобный интерфейс для анализа по тегам, индексам и временным диапазонам.

#### 8.3. Grafana и Alertmanager

- **Grafana** — платформа для визуализации данных, интегрированная с Prometheus, Elasticsearch и PostgreSQL для комплексного представления метрик и логов.
- **Alertmanager** — компонент, отвечающий за маршрутизацию и отправку оповещений (алертов) по электронной почте и в Telegram на основе заданных правил.

---

### Глава 9: Автоматизация и GitOps: Оптимизация процессов развертывания

В StreamForge реализован подход GitOps, который обеспечивает автоматизацию и упрощение процессов развертывания и управления инфраструктурой, минимизируя ручное вмешательство и повышая надежность.

#### 9.1. GitLab Runner

CI/CD конвейер построен на базе GitLab CI и использует `kaniko` для безопасной сборки образов контейнеров без необходимости в Docker Daemon.

- Runner функционирует в `kubernetes` executor с `nodeSelector` на узле `k2w-9` для оптимального распределения ресурсов.
- Поддерживается кэширование на базе Minio для ускорения процесса сборки.
- Конфигурация CI/CD разделена на общие шаблоны (`.build_python_service`) и специфичные пайплайны для каждого сервиса, обеспечивая модульность и переиспользование.

##### 9.1.1. Runner: особенности конфигурации

- Привилегированные права
- ServiceAccount: `full-access-sa`
- Пулы: `runner-home`, `docker-config`, `home-certificates`
- Репозиторий: `https://gitlab.dmz.home/`

##### 9.1.2. Структура пайплайна

- `setup` → `build` → `test` → `deploy`
- Используются include-файлы с путями к сервисам
- Переиспользуемые шаблоны `.gitlab/ci-templates/`

##### 9.1.3. Интеграция и модульность

Каждый сервис (например, `dummy-service`) использует переменные `SERVICE_NAME`, `SERVICE_PATH` и расширяет общий шаблон.

#### 9.2. ArgoCD

**ArgoCD** — это декларативный GitOps-инструмент, предназначенный для автоматизированного управления состоянием кластера Kubernetes на основе репозитория Git. Он обеспечивает:

- **Единый источник правды:** Репозиторий `iac_kubeadm` (`gitlab.dmz.home`) служит единственным источником истины для конфигурации кластера.
- **Поддержка TLS:** Обеспечивается безопасное взаимодействие с GitLab посредством TLS.
- **Веб-доступ:** Доступ к пользовательскому интерфейсу ArgoCD осуществляется по адресу `argocd.dmz.home`.
- **Контроль версий:** Все компоненты инфраструктуры находятся под версионным контролем, что упрощает отслеживание изменений и откат к предыдущим состояниям.

#### 9.3. Reloader

**Reloader** — это утилита, обеспечивающая автоматическую перезагрузку подов в Kubernetes при изменении связанных с ними `Secret` или `ConfigMap`. Это гарантирует, что приложения всегда используют актуальную конфигурацию без необходимости ручного вмешательства, поддерживая согласованность состояния развернутых компонентов.

---

### Глава 10: Безопасность и дополнительные возможности

#### 10.1. HashiCorp Vault

**HashiCorp Vault** используется в комбинации с `Vault CSI Driver` для безопасной и динамической поставки временных секретов в поды Kubernetes, исключая их постоянное хранение в кластере.

#### 10.2. Keycloak

**Keycloak** — это единый сервер аутентификации и управления доступом (IAM) для всех сервисов платформы. Он поддерживает стандарты SSO (Single Sign-On) и OpenID Connect, а также интегрируется с Grafana, Kibana и ArgoCD для централизованного управления пользователями и их правами.

#### 10.3. NVIDIA GPU Operator

**NVIDIA GPU Operator** позволяет автоматически обнаруживать и конфигурировать GPU в кластере Kubernetes, устраняя необходимость ручной установки драйверов.

- Версия: `v24.9.2`
- Поддержка GNN-тренировок: Обеспечивает необходимую инфраструктуру для эффективного обучения графовых нейронных сетей.
- Простота обновлений через Helm: Упрощает процесс обновления и управления оператором.

#### 10.4. Прочие утилиты

- `kubed` — утилита для синхронизации конфигураций между пространствами имен (namespaces) в Kubernetes, обеспечивающая консистентность настроек.
- `Mailrelay` — централизованная SMTP-шина, используемая для отправки нотификаций от различных компонентов системы, таких как Alertmanager, CronJob-ы и CI/CD пайплайны.
